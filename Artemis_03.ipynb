{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0msyID6sWWDx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from math import sqrt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense_Layer:\n",
        "    def __init__(self, input_size, n_neurons):\n",
        "        epsilon_init = 0.12\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 2 * epsilon_init - epsilon_init\n",
        "        self.input_size = input_size\n",
        "        self.n_neurons = n_neurons\n",
        "        self.w = np.random.RandomState(1).normal(loc=0.0, scale=0.1, size=(input_size, n_neurons))\n",
        "        self.b = np.ones((1, n_neurons))\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 0.01\n",
        "\n",
        "\n",
        "        # Adam parameters\n",
        "        #\n",
        "        self.Vdw_prev = 0\n",
        "        self.Vdb_prev = 0\n",
        "        self.Sdw_prev = 0\n",
        "        self.Sdb_prev = 0\n",
        "        self.t = 1\n",
        "\n",
        "        # epsilon_init = 0.12\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 2 * epsilon_init - epsilon_init\n",
        "\n",
        "\n",
        "    def forward(self, dense_input):\n",
        "        self.z = np.dot(dense_input, self.w) + self.b\n",
        "        return self.z\n",
        "\n",
        "    def output_layer_forward(self, dense_input):\n",
        "        self.z = np.dot(dense_input, self.w) + self.b\n",
        "\n",
        "        \n",
        "        return self.z\n",
        "\n",
        "    def dropout(self, activation):\n",
        "        self.keep_rate = 0.75\n",
        "        # self.dropout2_mask = np.random.randn(activation.shape[0], activation.shape[1]) < self.keep_rate\n",
        "\n",
        "        self.dropout_mask = np.ones(shape=(activation.shape[0], activation.shape[1])).reshape((1, -1)) == 1\n",
        "\n",
        "        # print(self.dropout_mask)\n",
        "        inactive_neurons = int(self.dropout_mask.shape[1] - (self.dropout_mask.shape[1] * self.keep_rate))\n",
        "        inactive_neurons_indices = sorted(random.sample(range(0, self.dropout_mask.shape[1]), inactive_neurons))\n",
        "\n",
        "        self.dropout_mask[0][inactive_neurons_indices] = 0\n",
        "        self.dropout_mask = self.dropout_mask.reshape(activation.shape[0], activation.shape[1])\n",
        "\n",
        "        # activation1 = (activation * self.dropout2_mask) / self.keep_rate\n",
        "        activation *= self.dropout_mask\n",
        "        return activation\n",
        "\n",
        "\n",
        "    def adam(self, B1, B2, EPSILON, dw, db, eta):\n",
        "        Vdw = B1 * self.Vdw_prev + (1 - B1) * dw\n",
        "        Vdb = B1 * self.Vdb_prev + (1 - B1) * db\n",
        "\n",
        "        # Vdw = Vdw / (1 - B1 ** self.t)\n",
        "        # Vdb = Vdb / (1 - B1 ** self.t)\n",
        "\n",
        "        self.Vdw_prev = Vdw\n",
        "        self.Vdb_prev = Vdb\n",
        "\n",
        "        Sdw = B2 * self.Sdw_prev + (1 - B2) * (dw ** 2)\n",
        "        Sdb = B2 * self.Sdb_prev + (1 - B2) * (db ** 2)\n",
        "\n",
        "        # Sdw = Sdw / (1 - B2 ** self.t)\n",
        "        # Sdb = Sdb / (1 - B2 ** self.t)\n",
        "\n",
        "        self.Sdw_prev = Sdw\n",
        "        self.Sdb_prev = Sdb\n",
        "\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        self.w -= eta * (Vdw / (sqrt(Sdw.sum()) + EPSILON))\n",
        "        self.b -= eta * (Vdb / (sqrt(Sdb.sum()) + EPSILON))\n",
        "\n",
        "    def adam_output(self, B1, B2, EPSILON, dw, db, eta):\n",
        "        Vdw = B1 * self.Vdw_prev + (1 - B1) * dw\n",
        "        Vdb = B1 * self.Vdb_prev + (1 - B1) * db\n",
        "\n",
        "        # Vdw = Vdw / (1 - B1 ** self.t)\n",
        "        # Vdb = Vdb / (1 - B1 ** self.t)\n",
        "\n",
        "        self.Vdw_prev = Vdw\n",
        "        self.Vdb_prev = Vdb\n",
        "\n",
        "        Sdw = B2 * self.Sdw_prev + (1 - B2) * (dw ** 2)\n",
        "        Sdb = B2 * self.Sdb_prev + (1 - B2) * (db ** 2)\n",
        "\n",
        "        # Sdw = Sdw / (1 - B2 ** self.t)\n",
        "        # Sdb = Sdb / (1 - B2 ** self.t)\n",
        "\n",
        "        self.Sdw_prev = Sdw\n",
        "        self.Sdb_prev = Sdb\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        self.w -= eta * (Vdw / (sqrt(Sdw.sum()) + EPSILON))\n",
        "        self.b -= eta * (Vdb / (sqrt(Sdb.sum()) + EPSILON))\n",
        "\n",
        "\n",
        "class AtlasNN:\n",
        "\n",
        "    def __init__(self, n_hidden=2, epochs=150, eta=0.005, l2=0.01, batch_size=5, seed=1, adapter_weights=None, adapter_bias=None):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.l2 = l2\n",
        "        self.random = np.random.RandomState(1)\n",
        "        self.adapter_weights = adapter_weights\n",
        "        self.adapter_bias = adapter_bias\n",
        "        if self.adapter_weights != None:\n",
        "          self.aaa = self.adapter_weights / np.sum(self.adapter_weights, axis=1, keepdims=True)\n",
        "        self.i = 0\n",
        "\n",
        "    def one_hot(self, y, n_classes):\n",
        "        onehot = np.zeros((n_classes, y.shape[0]))\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "          onehot[val, idx] = 1.\n",
        "        return onehot.T\n",
        "\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "      \n",
        "    def derivative_relu(self, x):\n",
        "        return np.array(x > 0,  dtype=np.float32)\n",
        "\n",
        "    def tanh(self, z):\n",
        "      return np.tanh(z)\n",
        "\n",
        "    def derivative_tanh(self, z):\n",
        "        return (1 - np.power(z, 2))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "    def derivative_sigmoid(self, z):\n",
        "        return z * (1 - z) \n",
        "    \n",
        "\n",
        "    def softmax(self, z, x_data, y_data, num_examples):\n",
        "        scores = z\n",
        "\n",
        "        # print('result matrix')\n",
        "        # print(scores.shape)\n",
        "\n",
        "        # u, s, vt = np.linalg.svd(scores)\n",
        "\n",
        "        # print('SVD properties')\n",
        "        # print(u.shape)\n",
        "        # print(s.shape)\n",
        "        # print(vt.shape)\n",
        "        # scores = z - np.max(z)\n",
        "\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # dscores = probs\n",
        "        # print(dscores[range(num_examples), y_data])\n",
        "        # dscores[range(num_examples), y_data] -= 1\n",
        "        # dscores /= num_examples\n",
        "\n",
        "        if self.adapter_weights != None:\n",
        "          if self.i == 0:\n",
        "            print('Activate Linear Adapter')\n",
        "            self.i+=1\n",
        "          probs = self.linear_adapter(probs, x_data, y_data)\n",
        "\n",
        "        # print('linear results')\n",
        "        # print(res)\n",
        "\n",
        "        return probs\n",
        "        # return res\n",
        "\n",
        "    def cross_entropy_derivative(self, nn,y):\n",
        "        nn = nn.clip(min=1e-8,max=None)\n",
        "        #print('\\n\\nCED: ', np.where(y==1,-1/X, 0))\n",
        "        return np.where(y==1,-1/nn, 0)\n",
        "\n",
        "    def knn_proba(self, knn, x_data, labels):\n",
        "        knn.fit(x_data, labels)\n",
        "        print(knn.kneighbors_graph(x_data).toarray().shape)\n",
        "        print(knn.get_params())\n",
        "        print('knn trained!!')\n",
        "\n",
        "    def linear_adapter(self, softmax_activation, x_data, y_data):\n",
        "\n",
        "        f_activation = []\n",
        "        result = []\n",
        "        \n",
        "        for sample in x_data:\n",
        "          array = np.sum(self.adapter_bias + np.dot(sample, self.aaa), axis=0)\n",
        "          f_activation.append([array[0], array[1], array[2]])\n",
        "        \n",
        "        f_function = np.array(f_activation).reshape((3, 3))\n",
        "        f_function /= np.sum(f_function, axis=1, keepdims=True)\n",
        "\n",
        "        result = np.zeros((3, 3))\n",
        "        for i in range(softmax_activation.shape[0]):\n",
        "           for j in range(softmax_activation.shape[1]):\n",
        "              result[i][j] = softmax_activation[i][j] * f_activation[i][j]\n",
        "        result /= np.sum(result, axis=1, keepdims=True)\n",
        "        return result\n",
        "\n",
        "    def fit(self, x_data, y_data, x_test_data, y_test_data):\n",
        "        self.h1_layer = Dense_Layer(x_data.shape[1], 100)\n",
        "        self.h2_layer = Dense_Layer(100, 100)\n",
        "        self.h3_layer = Dense_Layer(100, 50)\n",
        "        self.h4_layer = Dense_Layer(50, 50)\n",
        "        self.output_layer = Dense_Layer(50, 3)\n",
        "\n",
        "        step_size = self.eta\n",
        "        reg = 0.02\n",
        "\n",
        "        #Adam params\n",
        "        B1 = 0.9\n",
        "        B2 = 0.999\n",
        "        EPSILON = 1e-10\n",
        "        batch_size = 3#32\n",
        "\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "\n",
        "        # knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "        i = 0\n",
        "\n",
        "        y_train_one_hot = self.one_hot(y_data, 3)\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            indices = np.arange(x_data.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
        "              batch_idx = indices[idx:idx + batch_size]\n",
        "\n",
        "              z1 = self.h1_layer.forward(x_data[batch_idx])\n",
        "              h1 = self.sigmoid(z1)\n",
        "              # h1 = self.h1_layer.dropout(h1)\n",
        "\n",
        "              z2 = self.h2_layer.forward(h1)\n",
        "              h2 = self.sigmoid(z2)\n",
        "              h2 = self.h2_layer.dropout(h2)\n",
        "\n",
        "              z3 = self.h3_layer.forward(h2)\n",
        "              h3 = self.relu(z3)\n",
        "\n",
        "              z4 = self.h4_layer.forward(h3)\n",
        "              h4 = self.relu(z4)\n",
        "              h4 = self.h4_layer.dropout(h4)\n",
        "\n",
        "              z5 = self.output_layer.output_layer_forward(h4)\n",
        "\n",
        "              # if i == 0:\n",
        "              #   self.knn_proba(knn, x_data[batch_idx], y_data[batch_idx])\n",
        "              #   i=i+1\n",
        "\n",
        "              dscores = self.softmax(z5, x_data[batch_idx], y_data[batch_idx], x_data[batch_idx].shape[0])\n",
        "\n",
        "              ###\n",
        "              ##\n",
        "              #   BACKPROPAGATION\n",
        "              ##\n",
        "              ###\n",
        "              # print('dscores')\n",
        "              # print(dscores)\n",
        "              # print('real')\n",
        "              # print(y_train_one_hot[batch_idx])\n",
        "              dz_out = dscores - y_train_one_hot[batch_idx]\n",
        "              dw_out = np.dot(h4.T, dz_out)\n",
        "              db_out = np.sum(dscores, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "              dz4 = np.dot(dz_out, self.output_layer.w.T) * self.derivative_relu(h4)\n",
        "              dz4 = dz4 * self.h4_layer.dropout_mask\n",
        "              dw4 = np.dot(h3.T, dz4)\n",
        "              db4 = np.sum(dz4, axis=0, keepdims=True)\n",
        "\n",
        "              dz3 = np.dot(dz4, self.h4_layer.w.T) * self.derivative_relu(h3)\n",
        "              dw3 = np.dot(h2.T, dz3)\n",
        "              db3 = np.sum(dz3, axis=0, keepdims=True)\n",
        "\n",
        "              ###### hidden layer error #####\n",
        "              dz2 = np.dot(dz3, self.h3_layer.w.T) * self.derivative_sigmoid(h2)\n",
        "              # dh2[h2 <= 0] = 0\n",
        "\n",
        "              # dh2 = dh2 * self.h2_layer.dropout_mask / self.h2_layer.keep_rate\n",
        "              # self.h2_layer.dropout(dh2)\n",
        "\n",
        "              #### !!!! dropout in backpropagation !!!!! ######\n",
        "              dz2 = dz2 * self.h2_layer.dropout_mask\n",
        "              dw2 = np.dot(h1.T, dz2)\n",
        "              db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "              ### -----> should be after of before gradient calculation <----- ####\n",
        "              # dh2 = dh2 * self.h2_layer.dropout_mask\n",
        "\n",
        "              ###### hidden layer error #####\n",
        "              dz1 = np.dot(dz2, self.h2_layer.w.T) * self.derivative_sigmoid(h1)\n",
        "              # dh1[dh1 <= 0] = 0\n",
        "\n",
        "              # dh1 = dh1 * self.h1_layer.dropout_mask / self.h1_layer.keep_rate\n",
        "              # dh1 = dh1 * self.h1_layer.dropout_mask\n",
        "              # self.h1_layer.dropout(dh1)\n",
        "\n",
        "              dw1 = np.dot(x_data[batch_idx].T, dz1)\n",
        "              db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "              # without - elipse decision boundary, with - straight line\n",
        "              # dw_out += self.l2 * self.output_layer.w\n",
        "              # dw2 += self.l2 * self.h2_layer.w\n",
        "\n",
        "              self.h1_layer.adam_output(B1, B2, EPSILON, dw1, db1, step_size)\n",
        "\n",
        "              self.h2_layer.adam(B1, B2, EPSILON, dw2, db2, step_size)\n",
        "\n",
        "              self.h3_layer.adam(B1, B2, EPSILON, dw3, db3, step_size)\n",
        "\n",
        "              self.h4_layer.adam(B1, B2, EPSILON, dw4, db4, step_size)\n",
        "\n",
        "              self.output_layer.adam(B1, B2, EPSILON, dw_out, db_out, step_size)\n",
        "\n",
        "            # train_pred = self.predict(x_data)\n",
        "            # train_acc = accuracy_score(train_pred, y_train)\n",
        "            # self.train_scores.append(train_acc)\n",
        "\n",
        "            # test_pred = self.predict(x_test_data)\n",
        "            # test_acc = accuracy_score(test_pred, y_test)\n",
        "            # self.test_scores.append(test_acc)\n",
        "            # print('accuracy: %.2f' % (np.mean(predicted_class == y_test_data)))\n",
        "\n",
        "    def predict(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        z3 = self.h3_layer.forward(h2)\n",
        "        h3 = self.relu(z3)\n",
        "\n",
        "        z4 = self.h4_layer.forward(h3)\n",
        "        h4 = self.relu(z4)\n",
        "\n",
        "        scores = self.output_layer.forward(h4)\n",
        "\n",
        "        predicted_class = np.argmax(scores, axis=1)\n",
        "        return predicted_class\n",
        "\n",
        "    def predict_scores(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        z3 = self.h3_layer.forward(h2)\n",
        "        h3 = self.relu(z3)\n",
        "\n",
        "        z4 = self.h4_layer.forward(h3)\n",
        "        h4 = self.relu(z4)\n",
        "\n",
        "        return self.output_layer.forward(h4)\n",
        "\n",
        "    def plot_decision_regions(self, x, y, test_idx=None, resolution=0.02):\n",
        "        markers = ('s', 'x', 'o', '^', 'v')\n",
        "        colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "        # x = x[:, :2]\n",
        "        x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "        x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                               np.arange(x2_min, x2_max, resolution))\n",
        "        z = self.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "        z = z.reshape(xx1.shape)\n",
        "        plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "        plt.xlim(xx1.min(), xx1.max())\n",
        "        plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "        a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "        for idx, cl in enumerate(np.unique(y)):\n",
        "            plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "        plt.legend(loc='upper left')\n"
      ],
      "metadata": {
        "id": "kdm7ln8WICh8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "std = StandardScaler()\n",
        "x = std.fit_transform(x)"
      ],
      "metadata": {
        "id": "DoHezBW02OIM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "\n",
        "def plot(x, y, lr, test_idx=None, resolution=0.02):\n",
        "  markers = ('s', 'x', 'o', '^', 'v')\n",
        "  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "  cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "  # x = x[:, :2]\n",
        "  x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "  x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "  np.arange(x2_min, x2_max, resolution))\n",
        "  z = lr.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "  z = z.reshape(xx1.shape)\n",
        "  plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "  plt.xlim(xx1.min(), xx1.max())\n",
        "  plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "  a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "  for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "    edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "\n",
        "# x_train = x[:100, [0, 2]]\n",
        "# y_train = y[:100]\n",
        "\n",
        "x_train = x[:150, [0, 2, 3]]\n",
        "y_train = y[:150]\n",
        "\n",
        "labels = np.unique(y_train)\n",
        "\n",
        "# epoch = 100\n",
        "# atlasNN = AtlasNN(eta=0.11, epochs=epoch)\n",
        "\n",
        "two_class_labels = []\n",
        "weights_list = []\n",
        "lr = LogisticRegression(C=1000.0, random_state=1)\n",
        "clf = svm.SVC()\n",
        "melted_weights = []\n",
        "bias = []\n",
        "\n",
        "for idx, val in enumerate(labels):\n",
        "  two_class_labels = []\n",
        "  for i in range(y.shape[0]):\n",
        "    if y[i] == val:\n",
        "      two_class_labels.append(1)\n",
        "    else:\n",
        "      two_class_labels.append(0)\n",
        "  if val == 1:\n",
        "    clf.fit(x_train, two_class_labels)\n",
        "    print(np.sum(clf.support_vectors_, axis=0))\n",
        "    # plot(x_train, two_class_labels, clf)\n",
        "    # plt.show()\n",
        "    melted_weights.append(np.sum(clf.support_vectors_, axis=0))\n",
        "    bias.append(clf.intercept_)\n",
        "  else:\n",
        "    lr.fit(x_train, two_class_labels)\n",
        "    print(lr.coef_.ravel())\n",
        "    # plot(x_train, two_class_labels, lr)\n",
        "    # plt.show()\n",
        "    melted_weights.append(lr.coef_.ravel())\n",
        "    bias.append(lr.intercept_)\n",
        "\n",
        "\n",
        "print(np.array(bias).shape)\n",
        "print(x[149].shape)\n",
        "print(np.array(melted_weights).shape)\n",
        "print('Result')\n",
        "# print((bias + np.dot(x[149, [0, 1, 2]].T, melted_weights)))\n",
        "\n",
        "# q = np.array([[-16.44300347, -20.3190727,  -22.56697013],\n",
        "#               [-16.01201568, -19.6517545,  -24.43155689],\n",
        "#               [-15.78711386, -20.87569303, -24.23575341],\n",
        "#               [-21.21341109, -15.57864768, -23.97031816],\n",
        "#               [-20.433138,   -16.82906322, -22.26475414]])\n",
        "q = np.array([[-16.44300347, -20.3190727,  -22.56697013],\n",
        "              [-16.01201568, -19.6517545,  -24.43155689]])\n",
        "\n",
        "for sample in q:\n",
        "  print(sample)\n",
        "  print(np.sum(bias + np.dot(x[23, [0, 1, 2]], melted_weights), axis=0))\n",
        "# print(lr.coef_)\n",
        "# print(np.sum(clf.support_vectors_, axis=0))\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "\n",
        "plt.scatter(x_train[:50, 0], x_train[:50, 1],\n",
        "            color='blue', marker='o', label='Setosa')\n",
        "plt.scatter(x_train[50:100, 0], x_train[50:100, 1],\n",
        "            color='green', marker='s', label='Versicolor')\n",
        "plt.scatter(x_train[100:150, 0], x_train[100:150, 1],\n",
        "            color='red', marker='x', label='Virginica')\n",
        " \n",
        " \n",
        "plt.xlabel('Sepal length [cm]')\n",
        "plt.ylabel('Petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        " \n",
        "# plt.savefig\n",
        "\n",
        "# clf.fit(x_train, two_class_labels)\n",
        "# plot(x_train, two_class_labels, clf)\n",
        "# plt.show()\n",
        "\n",
        "# lr.fit(x_train, two_class_labels)\n",
        "# plot(x_train, two_class_labels, lr)\n",
        "# plt.show()\n",
        "\n",
        "epoch=300\n",
        "# atlasNN = AtlasNN(eta=0.25019, epochs=epoch)\n",
        "# atlasNN = AtlasNN(eta=0.003, epochs=epoch)\n",
        "atlasNN = AtlasNN(eta=0.003, epochs=epoch, adapter_weights=None, adapter_bias=bias)\n",
        "\n",
        "# atlasNN.fit(x_train, np.array(two_class_labels), [], [], x_train.shape[1])\n",
        "\n",
        "# atlasNN.plot_decision_regions(x_train, two_class_labels)\n",
        "# plt.show()\n",
        "\n",
        "atlasNN.fit(x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "# atlasNN.fit(x, y, [], [], x_train.shape[1])\n",
        "# atlasNN.plot_decision_regions(x_train, y_train)\n",
        "# plt.show()\n",
        "\n",
        "print('Train: ', np.sum(atlasNN.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test: ', np.sum(atlasNN.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "dI57OZO6FazD",
        "outputId": "adf7e83c-edef-4e46-dcef-1cda403dd99b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.01827133 -8.54939834 -7.34949946]\n",
            "[ 9.87922037 17.105792   14.64451928]\n",
            "[-3.00745256 16.18145695  7.50315504]\n",
            "(3, 1)\n",
            "(4,)\n",
            "(3, 3)\n",
            "Result\n",
            "[-16.44300347 -20.3190727  -22.56697013]\n",
            "[  2.23146722 -29.94315553  -6.85661514]\n",
            "[-16.01201568 -19.6517545  -24.43155689]\n",
            "[  2.23146722 -29.94315553  -6.85661514]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.9333333333333333\n",
            "Test:  0.9777777777777777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:132: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:133: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Destiller():\n",
        "  def __init__(self, teacher, eta, epochs, adapter_weights, adapter_bias):\n",
        "    self.teacher = teacher\n",
        "    self.eta = eta\n",
        "    self.epochs = epochs\n",
        "    self.adapter_bias = adapter_bias\n",
        "    self.adapter_weights = adapter_weights\n",
        "    if self.adapter_weights != None:\n",
        "      self.aaa = self.adapter_weights / np.sum(self.adapter_weights, axis=1, keepdims=True)\n",
        "    self.i = 0\n",
        "\n",
        "  def predict(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        scores = self.output_layer.forward(h2)\n",
        "\n",
        "        predicted_class = np.argmax(scores, axis=1)\n",
        "        return predicted_class\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "      return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "  def derivative_sigmoid(self, z):\n",
        "      return z * (1 - z)\n",
        "\n",
        "  def softmax(self, z, x_data=None, y_data=None):\n",
        "        scores = z\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        if self.adapter_weights != None:\n",
        "          if self.i == 0:\n",
        "            print('Activate Linear Adapter')\n",
        "            self.i+=1\n",
        "          probs = self.linear_adapter(probs, x_data, y_data)\n",
        "\n",
        "        return probs\n",
        "\n",
        "  def linear_adapter(self, softmax_activation, x_data, y_data):\n",
        "\n",
        "        f_activation = []\n",
        "        result = []\n",
        "        \n",
        "        for sample in x_data:\n",
        "          array = np.sum(self.adapter_bias + np.dot(sample, self.aaa), axis=0)\n",
        "          f_activation.append([array[0], array[1], array[2]])\n",
        "        \n",
        "        f_function = np.array(f_activation).reshape((3, 3))\n",
        "        f_function /= np.sum(f_function, axis=1, keepdims=True)\n",
        "\n",
        "        result = np.zeros((3, 3))\n",
        "        for i in range(softmax_activation.shape[0]):\n",
        "           for j in range(softmax_activation.shape[1]):\n",
        "              result[i][j] = softmax_activation[i][j] * f_activation[i][j]\n",
        "        result /= np.sum(result, axis=1, keepdims=True)\n",
        "        return result\n",
        "\n",
        "\n",
        "  def student_loss_function(self, student_predictions, y):\n",
        "    predicted_class = np.argmax(student_predictions, axis=1)\n",
        "    return -y * np.log(student_predictions)\n",
        "\n",
        "\n",
        "  def fit(self, x_data, y_data, temperature, alpha):\n",
        "\n",
        "    # self.student.fit(x, y, x.shape[1])\n",
        "\n",
        "\n",
        "    self.h1_layer = Dense_Layer(x_data.shape[1], 100)\n",
        "    self.h2_layer = Dense_Layer(100, 50)\n",
        "    self.output_layer = Dense_Layer(50, 3)\n",
        "\n",
        "    step_size = self.eta\n",
        "    reg = 0.02\n",
        "\n",
        "        #Adam params\n",
        "    B1 = 0.9\n",
        "    B2 = 0.999\n",
        "    EPSILON = 1e-10\n",
        "    batch_size = 3#32\n",
        "\n",
        "    self.train_scores = []\n",
        "    self.test_scores = []\n",
        "    i = 0\n",
        "\n",
        "    y_train_one_hot = self.teacher.one_hot(y_data, 3)\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "\n",
        "        indices = np.arange(x_data.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
        "          batch_idx = indices[idx:idx + batch_size]\n",
        "\n",
        "          teacher_predictions = self.teacher.one_hot(self.teacher.predict(x_data[batch_idx]), 3)\n",
        "\n",
        "          z1 = self.h1_layer.forward(x_data[batch_idx])\n",
        "          h1 = self.sigmoid(z1)\n",
        "\n",
        "          z2 = self.h2_layer.forward(h1)\n",
        "          h2 = self.sigmoid(z2)\n",
        "          h2 = self.h2_layer.dropout(h2)\n",
        "\n",
        "          z3 = self.output_layer.output_layer_forward(h2)\n",
        "\n",
        "          dscores = self.softmax(z3, x_data=x_data[batch_idx], y_data=y_data[batch_idx])\n",
        "\n",
        "          student_loss = self.student_loss_function(dscores, y_train_one_hot[batch_idx])\n",
        "\n",
        "          # print('calculate loss')\n",
        "\n",
        "\n",
        "          # distillation_loss = self.softmax(teacher_predictions / temperature) * np.log(self.softmax(teacher_predictions / temperature)\n",
        "          #     /self.softmax(dscores / temperature))\n",
        "          \n",
        "\n",
        "          # print(teacher_predictions.shape)\n",
        "          # print(dscores.shape)\n",
        "          # distillation_loss = (self.softmax(teacher_predictions / temperature) - self.softmax(dscores / temperature)) / temperature\n",
        "          # print('loss calculated')\n",
        "\n",
        "\n",
        "          # loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "          dz_out = dscores - teacher_predictions#dscores - y_train_one_hot[batch_idx]\n",
        "          dw_out = np.dot(h2.T, dz_out)\n",
        "          db_out = np.sum(dscores, axis=0, keepdims=True)\n",
        "\n",
        "          dz2 = np.dot(dz_out, self.output_layer.w.T) * self.derivative_sigmoid(h2)\n",
        "          dz2 = dz2 * self.h2_layer.dropout_mask\n",
        "          dw2 = np.dot(h1.T, dz2)\n",
        "          db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "          dz1 = np.dot(dz2, self.h2_layer.w.T) * self.derivative_sigmoid(h1)\n",
        "          dw1 = np.dot(x_data[batch_idx].T, dz1)\n",
        "          db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "          self.h1_layer.adam_output(B1, B2, EPSILON, dw1, db1, step_size)\n",
        "\n",
        "          self.h2_layer.adam(B1, B2, EPSILON, dw2, db2, step_size)\n",
        "\n",
        "          self.output_layer.adam(B1, B2, EPSILON, dw_out, db_out, step_size)\n",
        "\n",
        "          # distillation_loss = (self.teacher.softmax_activation(teacher_predictions / temperature) - self.student.softmax_activation(student_predictions / temperature)) / temperature\n",
        "          # student_predictions = self.student.predict(x)\n",
        "          # student_loss = self.student_loss_function(student_predictions, y)\n",
        "\n",
        "          # distillation_loss = (self.teacher.softmax_activation(teacher_predictions / temperature) - self.student.softmax_activation(student_predictions / temperature)) / temperature\n",
        "\n",
        "          # loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "\n",
        "  def plot_decision_regions(self, x, y, test_idx=None, resolution=0.02):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    pca = PCA(n_components = 2)\n",
        "    x = pca.fit_transform(x[:, [0, 1]])\n",
        "\n",
        "        # x = x[:, :2]\n",
        "    x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                                np.arange(x2_min, x2_max, resolution))\n",
        "    z = self.predict(np.array([xx1.ravel(), xx2.ravel(), xx3.ravel()]).T)\n",
        "    z = z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "  def plot_compressed_decision_regions(self, x, y, test_idx=None, resolution=2):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    pca = PCA(n_components = 2)\n",
        "\n",
        "    # x has features [0, 2, 3], first we merge 0, 2 as one feature then 2, 3 and last 0, 3\n",
        "\n",
        "    x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    x3_min, x3_max = x[:, 2].min() - 1, x[:, 2].max() + 1\n",
        "    xx1, xx2, xx3 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                                np.arange(x2_min, x2_max, resolution),\n",
        "                                np.arange(x3_min, x3_max, resolution))\n",
        "    z = self.predict(np.array([xx1.ravel(), xx2.ravel(), xx3.ravel()]).T)\n",
        "\n",
        "    print(xx1)\n",
        "    print(xx2.shape)\n",
        "    xx1 = np.reshape(xx1, (3, 12))\n",
        "    xx2 = np.reshape(xx2, (3, 12))\n",
        "    z = z.reshape(xx1.shape)\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.contour3D(xx1, xx2, z, cmap=cmap)\n",
        "    plt.show()\n",
        "\n",
        "    xx3 = np.reshape(xx3, (3, 12))\n",
        "    z = z.reshape(xx2.shape)\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.contour3D(xx2, xx3, z, cmap=cmap)\n",
        "    plt.show()\n",
        "\n",
        "    z = z.reshape(xx1.shape)\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.contour3D(xx1, xx3, z, cmap=cmap)\n",
        "    plt.show()\n",
        "    # z = z.reshape(xx1.shape)\n",
        "    # plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "    # plt.xlim(xx1.min(), xx1.max())\n",
        "    # plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    # for idx, cl in enumerate(np.unique(y)):\n",
        "    #   plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "    #                       edgecolors='black')\n",
        "    # plt.legend(loc='upper left')\n",
        "    # plt.show()\n",
        "\n",
        "    # z = z.reshape(xx2.shape)\n",
        "    # plt.contourf(xx2, xx3, z, alpha=0.4, cmap=cmap)\n",
        "    # plt.xlim(xx2.min(), xx2.max())\n",
        "    # plt.ylim(xx3.min(), xx3.max())\n",
        "\n",
        "    # a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    # for idx, cl in enumerate(np.unique(y)):\n",
        "    #   plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "    #                       edgecolors='black')\n",
        "    # plt.legend(loc='upper left')\n",
        "    # plt.show()\n"
      ],
      "metadata": {
        "id": "_4rVjS1jSg5B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "destiller = Destiller(atlasNN, eta=0.004, epochs=200, adapter_weights=None, adapter_bias=bias)\n",
        "\n",
        "destiller.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "# destiller.plot_compressed_decision_regions(x_train, y_train)\n",
        "\n",
        "print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])"
      ],
      "metadata": {
        "id": "h3Fnxx2-z0Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowa sekcja"
      ],
      "metadata": {
        "id": "oi-zM0143BCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "print(np.sum(a, axis = 1, keepdims = True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TCS1afS8pgi",
        "outputId": "c898a837-1272-4ca3-d2c2-3161bf8f9ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[[ 6]\n",
            " [15]]\n"
          ]
        }
      ]
    }
  ]
}