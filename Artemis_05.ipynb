{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0msyID6sWWDx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from math import sqrt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense_Layer:\n",
        "    def __init__(self, input_size, n_neurons):\n",
        "        epsilon_init = 0.12\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 2 * epsilon_init - epsilon_init\n",
        "        self.input_size = input_size\n",
        "        self.n_neurons = n_neurons\n",
        "        self.w = np.random.RandomState(1).normal(loc=0.0, scale=0.1, size=(input_size, n_neurons))\n",
        "        self.b = np.ones((1, n_neurons))\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 0.01\n",
        "\n",
        "\n",
        "        # Adam parameters\n",
        "        #\n",
        "        self.Vdw_prev = 0\n",
        "        self.Vdb_prev = 0\n",
        "        self.Sdw_prev = 0\n",
        "        self.Sdb_prev = 0\n",
        "        self.t = 1\n",
        "\n",
        "        # epsilon_init = 0.12\n",
        "        # self.w = np.random.rand(input_size, n_neurons) * 2 * epsilon_init - epsilon_init\n",
        "\n",
        "\n",
        "    def forward(self, dense_input):\n",
        "        self.z = np.dot(dense_input, self.w) + self.b\n",
        "        return self.z\n",
        "\n",
        "    def output_layer_forward(self, dense_input, adapter_bias=None):\n",
        "        # print(self.b)\n",
        "        # print(np.array(adapter_bias).ravel())\n",
        "        # print(np.array(adapter_bias).ravel()*2)\n",
        "        if adapter_bias != None:\n",
        "          self.z = np.dot(dense_input, self.w) + ((self.b - np.array(adapter_bias).ravel()*0.5))\n",
        "          # self.z = np.dot(dense_input, self.w) + ((self.b + adapter_bias) * (1/2))\n",
        "        else:\n",
        "          self.z = np.dot(dense_input, self.w) + self.b * 0.5\n",
        "\n",
        "        \n",
        "        return self.z\n",
        "\n",
        "    def dropout(self, activation):\n",
        "        self.keep_rate = 0.75\n",
        "        # self.dropout2_mask = np.random.randn(activation.shape[0], activation.shape[1]) < self.keep_rate\n",
        "\n",
        "        self.dropout_mask = np.ones(shape=(activation.shape[0], activation.shape[1])).reshape((1, -1)) == 1\n",
        "\n",
        "        # print(self.dropout_mask)\n",
        "        inactive_neurons = int(self.dropout_mask.shape[1] - (self.dropout_mask.shape[1] * self.keep_rate))\n",
        "        inactive_neurons_indices = sorted(random.sample(range(0, self.dropout_mask.shape[1]), inactive_neurons))\n",
        "\n",
        "        self.dropout_mask[0][inactive_neurons_indices] = 0\n",
        "        self.dropout_mask = self.dropout_mask.reshape(activation.shape[0], activation.shape[1])\n",
        "\n",
        "        # activation1 = (activation * self.dropout2_mask) / self.keep_rate\n",
        "        activation *= self.dropout_mask\n",
        "        return activation\n",
        "\n",
        "\n",
        "    def adam(self, B1, B2, EPSILON, dw, db, eta):\n",
        "        Vdw = B1 * self.Vdw_prev + (1 - B1) * dw\n",
        "        Vdb = B1 * self.Vdb_prev + (1 - B1) * db\n",
        "\n",
        "        # Vdw = Vdw / (1 - B1 ** self.t)\n",
        "        # Vdb = Vdb / (1 - B1 ** self.t)\n",
        "\n",
        "        self.Vdw_prev = Vdw\n",
        "        self.Vdb_prev = Vdb\n",
        "\n",
        "        Sdw = B2 * self.Sdw_prev + (1 - B2) * (dw ** 2)\n",
        "        Sdb = B2 * self.Sdb_prev + (1 - B2) * (db ** 2)\n",
        "\n",
        "        # Sdw = Sdw / (1 - B2 ** self.t)\n",
        "        # Sdb = Sdb / (1 - B2 ** self.t)\n",
        "\n",
        "        self.Sdw_prev = Sdw\n",
        "        self.Sdb_prev = Sdb\n",
        "\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        self.w -= eta * (Vdw / (sqrt(Sdw.sum()) + EPSILON))\n",
        "        self.b -= eta * (Vdb / (sqrt(Sdb.sum()) + EPSILON))\n",
        "\n",
        "    def adam_output(self, B1, B2, EPSILON, dw, db, eta):\n",
        "        Vdw = B1 * self.Vdw_prev + (1 - B1) * dw\n",
        "        Vdb = B1 * self.Vdb_prev + (1 - B1) * db\n",
        "\n",
        "        # Vdw += 0.001\n",
        "        # Vdw = Vdw / (1 - B1 ** self.t)\n",
        "        # Vdb = Vdb / (1 - B1 ** self.t)\n",
        "\n",
        "        self.Vdw_prev = Vdw\n",
        "        self.Vdb_prev = Vdb\n",
        "\n",
        "        Sdw = B2 * self.Sdw_prev + (1 - B2) * (dw ** 2)\n",
        "        Sdb = B2 * self.Sdb_prev + (1 - B2) * (db ** 2)\n",
        "\n",
        "        # Sdw = Sdw / (1 - B2 ** self.t)\n",
        "        # Sdb = Sdb / (1 - B2 ** self.t)\n",
        "\n",
        "        self.Sdw_prev = Sdw\n",
        "        self.Sdb_prev = Sdb\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "        self.w -= eta * (Vdw / (sqrt(Sdw.sum()) + EPSILON))\n",
        "        self.b -= eta * (Vdb / (sqrt(Sdb.sum()) + EPSILON))\n",
        "\n",
        "\n",
        "class AtlasNN:\n",
        "\n",
        "    def __init__(self, n_hidden=2, epochs=150, eta=0.005, l2=0.01, batch_size=5, seed=1, adapter_weights=None, adapter_bias=None):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.eta = eta\n",
        "        self.l2 = l2\n",
        "        self.random = np.random.RandomState(1)\n",
        "        self.adapter_weights = adapter_weights\n",
        "        self.adapter_bias = adapter_bias\n",
        "        if self.adapter_weights != None:\n",
        "          self.aaa = self.adapter_weights / np.sum(self.adapter_weights, axis=1, keepdims=True)\n",
        "        self.i = 0\n",
        "\n",
        "    def one_hot(self, y, n_classes):\n",
        "        onehot = np.zeros((n_classes, y.shape[0]))\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "          onehot[val, idx] = 1.\n",
        "        return onehot.T\n",
        "\n",
        "\n",
        "    def relu(self, z):\n",
        "        return np.maximum(0, z)\n",
        "      \n",
        "    def derivative_relu(self, x):\n",
        "        return np.array(x > 0,  dtype=np.float32)\n",
        "\n",
        "    def tanh(self, z):\n",
        "      return np.tanh(z)\n",
        "\n",
        "    def derivative_tanh(self, z):\n",
        "        return (1 - np.power(z, 2))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "    def derivative_sigmoid(self, z):\n",
        "        return z * (1 - z) \n",
        "    \n",
        "\n",
        "    def softmax(self, z, x_data, y_data, num_examples):\n",
        "        scores = z\n",
        "\n",
        "        # print('result matrix')\n",
        "        # print(scores.shape)\n",
        "\n",
        "        # u, s, vt = np.linalg.svd(scores)\n",
        "\n",
        "        # print('SVD properties')\n",
        "        # print(u.shape)\n",
        "        # print(s.shape)\n",
        "        # print(vt.shape)\n",
        "        # scores = z - np.max(z)\n",
        "\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # dscores = probs\n",
        "        # print(dscores[range(num_examples), y_data])\n",
        "        # dscores[range(num_examples), y_data] -= 1\n",
        "        # dscores /= num_examples\n",
        "\n",
        "        if self.adapter_weights != None:\n",
        "          if self.i == 0:\n",
        "            print('Activate Linear Adapter')\n",
        "            self.i+=1\n",
        "          probs = self.linear_adapter(probs, x_data, y_data)\n",
        "\n",
        "        # print('linear results')\n",
        "        # print(res)\n",
        "\n",
        "        return probs\n",
        "        # return res\n",
        "\n",
        "    def cross_entropy_derivative(self, nn,y):\n",
        "        nn = nn.clip(min=1e-8,max=None)\n",
        "        #print('\\n\\nCED: ', np.where(y==1,-1/X, 0))\n",
        "        return np.where(y==1,-1/nn, 0)\n",
        "\n",
        "    def knn_proba(self, knn, x_data, labels):\n",
        "        knn.fit(x_data, labels)\n",
        "        print(knn.kneighbors_graph(x_data).toarray().shape)\n",
        "        print(knn.get_params())\n",
        "        print('knn trained!!')\n",
        "\n",
        "    def linear_adapter(self, softmax_activation, x_data, y_data):\n",
        "\n",
        "        f_activation = []\n",
        "        result = []\n",
        "        \n",
        "        for sample in x_data:\n",
        "          array = np.sum(self.adapter_bias + np.dot(sample, self.aaa.T), axis=0)\n",
        "          f_activation.append([array[0], array[1], array[2]])\n",
        "        \n",
        "        # print(f_activation)\n",
        "        f_function = np.array(f_activation).reshape((3, 3))\n",
        "        # f_function /= np.sum(f_function, axis=1, keepdims=True)\n",
        "\n",
        "        result = np.zeros((3, 3))\n",
        "        # for i in range(softmax_activation.shape[0]):\n",
        "        #    for j in range(softmax_activation.shape[1]):\n",
        "        #       result[i][j] = softmax_activation[i][j] * f_activation[i][j]\n",
        "        result = np.dot(softmax_activation, f_activation)\n",
        "        # result /= np.sum(result, axis=1, keepdims=True)\n",
        "        return result\n",
        "\n",
        "    def fit(self, x_data, y_data, x_test_data, y_test_data):\n",
        "        self.h1_layer = Dense_Layer(x_data.shape[1], 100)\n",
        "        self.h2_layer = Dense_Layer(100, 100)\n",
        "        self.h3_layer = Dense_Layer(100, 50)\n",
        "        self.h4_layer = Dense_Layer(50, 50)\n",
        "        self.output_layer = Dense_Layer(50, 3)\n",
        "\n",
        "        step_size = self.eta\n",
        "        reg = 0.02\n",
        "\n",
        "        #Adam params\n",
        "        B1 = 0.9\n",
        "        B2 = 0.999\n",
        "        EPSILON = 1e-10\n",
        "        batch_size = 3#32\n",
        "\n",
        "        self.train_scores = []\n",
        "        self.test_scores = []\n",
        "\n",
        "        # knn = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "        i = 0\n",
        "\n",
        "        y_train_one_hot = self.one_hot(y_data, 3)\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            indices = np.arange(x_data.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
        "              batch_idx = indices[idx:idx + batch_size]\n",
        "\n",
        "              z1 = self.h1_layer.forward(x_data[batch_idx])\n",
        "              h1 = self.sigmoid(z1)\n",
        "              # h1 = self.h1_layer.dropout(h1)\n",
        "\n",
        "              z2 = self.h2_layer.forward(h1)\n",
        "              h2 = self.sigmoid(z2)\n",
        "              h2 = self.h2_layer.dropout(h2)\n",
        "\n",
        "              z3 = self.h3_layer.forward(h2)\n",
        "              h3 = self.relu(z3)\n",
        "\n",
        "              z4 = self.h4_layer.forward(h3)\n",
        "              h4 = self.relu(z4)\n",
        "              h4 = self.h4_layer.dropout(h4)\n",
        "\n",
        "              z5 = self.output_layer.output_layer_forward(h4)\n",
        "\n",
        "              # if i == 0:\n",
        "              #   self.knn_proba(knn, x_data[batch_idx], y_data[batch_idx])\n",
        "              #   i=i+1\n",
        "\n",
        "              dscores = self.softmax(z5, x_data[batch_idx], y_data[batch_idx], x_data[batch_idx].shape[0])\n",
        "\n",
        "              ###\n",
        "              ##\n",
        "              #   BACKPROPAGATION\n",
        "              ##\n",
        "              ###\n",
        "              # print('dscores')\n",
        "              # print(dscores)\n",
        "              # print('real')\n",
        "              # print(y_train_one_hot[batch_idx])\n",
        "              dz_out = dscores - y_train_one_hot[batch_idx]\n",
        "              dw_out = np.dot(h4.T, dz_out)\n",
        "              db_out = np.sum(dscores, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "              dz4 = np.dot(dz_out, self.output_layer.w.T) * self.derivative_relu(h4)\n",
        "              dz4 = dz4 * self.h4_layer.dropout_mask\n",
        "              dw4 = np.dot(h3.T, dz4)\n",
        "              db4 = np.sum(dz4, axis=0, keepdims=True)\n",
        "\n",
        "              dz3 = np.dot(dz4, self.h4_layer.w.T) * self.derivative_relu(h3)\n",
        "              dw3 = np.dot(h2.T, dz3)\n",
        "              db3 = np.sum(dz3, axis=0, keepdims=True)\n",
        "\n",
        "              ###### hidden layer error #####\n",
        "              dz2 = np.dot(dz3, self.h3_layer.w.T) * self.derivative_sigmoid(h2)\n",
        "              # dh2[h2 <= 0] = 0\n",
        "\n",
        "              # dh2 = dh2 * self.h2_layer.dropout_mask / self.h2_layer.keep_rate\n",
        "              # self.h2_layer.dropout(dh2)\n",
        "\n",
        "              #### !!!! dropout in backpropagation !!!!! ######\n",
        "              dz2 = dz2 * self.h2_layer.dropout_mask\n",
        "              dw2 = np.dot(h1.T, dz2)\n",
        "              db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "              ### -----> should be after of before gradient calculation <----- ####\n",
        "              # dh2 = dh2 * self.h2_layer.dropout_mask\n",
        "\n",
        "              ###### hidden layer error #####\n",
        "              dz1 = np.dot(dz2, self.h2_layer.w.T) * self.derivative_sigmoid(h1)\n",
        "              # dh1[dh1 <= 0] = 0\n",
        "\n",
        "              # dh1 = dh1 * self.h1_layer.dropout_mask / self.h1_layer.keep_rate\n",
        "              # dh1 = dh1 * self.h1_layer.dropout_mask\n",
        "              # self.h1_layer.dropout(dh1)\n",
        "\n",
        "              dw1 = np.dot(x_data[batch_idx].T, dz1)\n",
        "              db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "              # without - elipse decision boundary, with - straight line\n",
        "              # dw_out += self.l2 * self.output_layer.w\n",
        "              # dw2 += self.l2 * self.h2_layer.w\n",
        "\n",
        "              self.h1_layer.adam_output(B1, B2, EPSILON, dw1, db1, step_size)\n",
        "\n",
        "              self.h2_layer.adam(B1, B2, EPSILON, dw2, db2, step_size)\n",
        "\n",
        "              self.h3_layer.adam(B1, B2, EPSILON, dw3, db3, step_size)\n",
        "\n",
        "              self.h4_layer.adam(B1, B2, EPSILON, dw4, db4, step_size)\n",
        "\n",
        "              self.output_layer.adam(B1, B2, EPSILON, dw_out, db_out, step_size)\n",
        "\n",
        "            # train_pred = self.predict(x_data)\n",
        "            # train_acc = accuracy_score(train_pred, y_train)\n",
        "            # self.train_scores.append(train_acc)\n",
        "\n",
        "            # test_pred = self.predict(x_test_data)\n",
        "            # test_acc = accuracy_score(test_pred, y_test)\n",
        "            # self.test_scores.append(test_acc)\n",
        "            # print('accuracy: %.2f' % (np.mean(predicted_class == y_test_data)))\n",
        "\n",
        "    def predict(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        z3 = self.h3_layer.forward(h2)\n",
        "        h3 = self.relu(z3)\n",
        "\n",
        "        z4 = self.h4_layer.forward(h3)\n",
        "        h4 = self.relu(z4)\n",
        "\n",
        "        scores = self.output_layer.forward(h4)\n",
        "\n",
        "        predicted_class = np.argmax(scores, axis=1)\n",
        "        return predicted_class\n",
        "\n",
        "    def predict_scores(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        z3 = self.h3_layer.forward(h2)\n",
        "        h3 = self.relu(z3)\n",
        "\n",
        "        z4 = self.h4_layer.forward(h3)\n",
        "        h4 = self.relu(z4)\n",
        "\n",
        "        return self.output_layer.forward(h4)\n",
        "\n",
        "    def plot_decision_regions(self, x, y, test_idx=None, resolution=0.05):\n",
        "        markers = ('s', 'x', 'o', '^', 'v')\n",
        "        colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "        cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "        # x = x[:, :2]\n",
        "        x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "        x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                               np.arange(x2_min, x2_max, resolution))\n",
        "        z = self.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "        z = z.reshape(xx1.shape)\n",
        "        plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "        plt.xlim(xx1.min(), xx1.max())\n",
        "        plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "        a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "        for idx, cl in enumerate(np.unique(y)):\n",
        "            plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "        plt.legend(loc='upper left')\n"
      ],
      "metadata": {
        "id": "kdm7ln8WICh8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "std = StandardScaler()\n",
        "x = std.fit_transform(x)"
      ],
      "metadata": {
        "id": "DoHezBW02OIM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "\n",
        "def plot(x, y, lr, test_idx=None, resolution=0.02):\n",
        "  markers = ('s', 'x', 'o', '^', 'v')\n",
        "  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "  cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "  # x = x[:, :2]\n",
        "  x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "  x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "  np.arange(x2_min, x2_max, resolution))\n",
        "  z = lr.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "  z = z.reshape(xx1.shape)\n",
        "  plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "  plt.xlim(xx1.min(), xx1.max())\n",
        "  plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "  a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "  for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "    edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "x_train = x[:150, [0, 2]]\n",
        "y_train = y[:150]\n",
        "\n",
        "labels = np.unique(y_train)\n",
        "\n",
        "# x_train = x[:100, [0, 2]]\n",
        "# y_train = y[:100]\n",
        "\n",
        "# epoch = 100\n",
        "# atlasNN = AtlasNN(eta=0.11, epochs=epoch)\n",
        "\n",
        "two_class_labels = []\n",
        "weights_list = []\n",
        "lr = LogisticRegression(C=1000.0, random_state=1)\n",
        "clf = svm.SVC()\n",
        "melted_weights = []\n",
        "bias = []\n",
        "\n",
        "for idx, val in enumerate(labels):\n",
        "  two_class_labels = []\n",
        "  for i in range(y.shape[0]):\n",
        "    if y[i] == val:\n",
        "      two_class_labels.append(1)\n",
        "    else:\n",
        "      two_class_labels.append(0)\n",
        "  if val == 1:\n",
        "    clf.fit(x_train, two_class_labels)\n",
        "    # print(np.sum(clf.support_vectors_, axis=0))\n",
        "    # plot(x_train, two_class_labels, clf)\n",
        "    # plt.show()\n",
        "    melted_weights.append(np.sum(clf.support_vectors_, axis=0))\n",
        "    bias.append(clf.intercept_)\n",
        "  else:\n",
        "    lr.fit(x_train, two_class_labels)\n",
        "    print(lr.coef_.ravel())\n",
        "    # plot(x_train, two_class_labels, lr)\n",
        "    # plt.show()\n",
        "    melted_weights.append(lr.coef_.ravel())\n",
        "    bias.append(lr.intercept_)\n",
        "\n",
        "\n",
        "print(np.array(bias).shape)\n",
        "print(x[149].shape)\n",
        "print(np.array(melted_weights).shape)\n",
        "print('Result')\n",
        "# print((bias + np.dot(x[149, [0, 1, 2]].T, melted_weights)))\n",
        "\n",
        "# q = np.array([[-16.44300347, -20.3190727,  -22.56697013],\n",
        "#               [-16.01201568, -19.6517545,  -24.43155689],\n",
        "#               [-15.78711386, -20.87569303, -24.23575341],\n",
        "#               [-21.21341109, -15.57864768, -23.97031816],\n",
        "#               [-20.433138,   -16.82906322, -22.26475414]])\n",
        "q = np.array([[-16.44300347, -20.3190727,  -22.56697013],\n",
        "              [-16.01201568, -19.6517545,  -24.43155689]])\n",
        "\n",
        "for sample in q:\n",
        "  print(sample)\n",
        "  print(np.sum(bias + np.dot(x[23, [0, 1, 2]], melted_weights), axis=0))\n",
        "# print(lr.coef_)\n",
        "# print(np.sum(clf.support_vectors_, axis=0))\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.4, random_state=1, stratify=y)\n",
        " \n",
        "# plt.savefig\n",
        "\n",
        "# clf.fit(x_train, two_class_labels)\n",
        "# plot(x_train, two_class_labels, clf)\n",
        "# plt.show()\n",
        "\n",
        "# lr.fit(x_train, two_class_labels)\n",
        "# plot(x_train, two_class_labels, lr)\n",
        "# plt.show()\n",
        "\n",
        "epoch=500\n",
        "# atlasNN = AtlasNN(eta=0.25019, epochs=epoch)\n",
        "# atlasNN = AtlasNN(eta=0.003, epochs=epoch)\n",
        "atlasNN = AtlasNN(eta=0.004, epochs=epoch)\n",
        "# atlasNN.fit(x_train, np.array(two_class_labels), [], [], x_train.shape[1])\n",
        "\n",
        "# atlasNN.plot_decision_regions(x_train, two_class_labels)\n",
        "# plt.show()\n",
        "atlasNN.fit(x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "# atlasNN.fit(x, y, [], [], x_train.shape[1])\n",
        "atlasNN.plot_decision_regions(x_test, y_test)\n",
        "plt.show()\n",
        "\n",
        "print('Train: ', np.sum(atlasNN.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test: ', np.sum(atlasNN.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "dI57OZO6FazD",
        "outputId": "541a62fa-21f3-4ef1-8fd2-320b0a11ebfa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -0.31514246 -14.95302602]\n",
            "[-3.16291632 22.41035329]\n",
            "(3, 1)\n",
            "(4,)\n",
            "(3, 2)\n",
            "Result\n",
            "[-16.44300347 -20.3190727  -22.56697013]\n",
            "[ 11.24029907 -23.14612535]\n",
            "[-16.01201568 -19.6517545  -24.43155689]\n",
            "[ 11.24029907 -23.14612535]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  0.9333333333333333\n",
            "Test:  0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ab925a4e660b>:115: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Train: ', np.sum(atlasNN.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
            "<ipython-input-4-ab925a4e660b>:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Test: ', np.sum(atlasNN.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Destiller():\n",
        "  def __init__(self, teacher, eta, epochs, adapter_weights=None, adapter_bias=None):\n",
        "    self.teacher = teacher\n",
        "    self.eta = eta\n",
        "    self.epochs = epochs\n",
        "    self.adapter_bias = adapter_bias\n",
        "    self.adapter_weights = adapter_weights\n",
        "    if self.adapter_weights != None:\n",
        "      self.normazlied_adapter_weights = self.adapter_weights / np.sum(self.adapter_weights, axis=1, keepdims=True)\n",
        "    self.i = 0\n",
        "    self.logistic_regression = LogisticRegression(C=1000.0, random_state=1)\n",
        "\n",
        "  def predict(self, x):\n",
        "        z1 = self.h1_layer.forward(x)\n",
        "        h1 = self.sigmoid(z1)\n",
        "\n",
        "        z2 = self.h2_layer.forward(h1)\n",
        "        h2 = self.sigmoid(z2)\n",
        "\n",
        "        scores = self.output_layer.forward(h2)\n",
        "\n",
        "        predicted_class = np.argmax(scores, axis=1)\n",
        "        return predicted_class\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "      return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "  def derivative_sigmoid(self, z):\n",
        "      return z * (1 - z)\n",
        "\n",
        "  def softmax(self, z, x_data=None, y_data=None):\n",
        "        scores = z\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # if self.adapter_weights != None:\n",
        "        #   if self.i == 0:\n",
        "        #     print('Activate Linear Adapter')\n",
        "        #     self.i+=1\n",
        "        #   probs = self.linear_adapter(probs, x_data, y_data)\n",
        "\n",
        "        if self.adapter_weights != None:\n",
        "          probs = probs - self.eta * self.artemis_linear_adapter(probs, x_data, y_data)\n",
        "        return probs\n",
        "\n",
        "  def linear_adapter(self, softmax_activation, x_data, y_data):\n",
        "\n",
        "        adapter_activation = []\n",
        "        result = []\n",
        "        \n",
        "        for sample in x_data:\n",
        "          array = np.sum(self.adapter_bias + np.dot(sample, self.normazlied_adapter_weights.T), axis=0)\n",
        "          adapter_activation.append([array[0], array[1], array[2]])\n",
        "        \n",
        "        adapter_activation = np.array(adapter_activation).reshape((3, 3))\n",
        "        # f_function /= np.sum(f_function, axis=1, keepdims=True)\n",
        "\n",
        "        result = np.zeros((3, 3))\n",
        "        for i in range(softmax_activation.shape[0]):\n",
        "           for j in range(softmax_activation.shape[1]):\n",
        "              result[i][j] = softmax_activation[i][j] * 1000 * adapter_activation[i][j]\n",
        "        result /= np.sum(result, axis=1, keepdims=True)\n",
        "\n",
        "        # return self.normazlied_adapter_weights\n",
        "        return result\n",
        "\n",
        "  def artemis_linear_adapter(self, softmax_activation, x_data, y_data):\n",
        "\n",
        "    logistic_regression_results = []\n",
        "    if len(np.unique(y_data) == 1):\n",
        "      return 1\n",
        "    for idx, unique_label in enumerate((np.unique(y_data))):\n",
        "      logistic_labels = []\n",
        "      for i, data_label in enumerate(y_data):\n",
        "        if data_label == unique_label:\n",
        "          logistic_labels.append(1)\n",
        "        else:\n",
        "          logistic_labels.append(0)\n",
        "      try:\n",
        "      # print(f'Labels for {unique_label}')\n",
        "      # print(y_data)\n",
        "      # print(logistic_labels)\n",
        "        # indexes_to_remove = []\n",
        "        # for idx, val in enumerate(logistic_labels):\n",
        "        #   first_label = val[0]\n",
        "        #   if len(val[val == first_label]) == len(val):\n",
        "        #     indexes_to_remove.append(idx)\n",
        "        \n",
        "        # filtered_data = np.delete(x_data, indexes_to_remove, axis=0)\n",
        "        # logistic_labels = np.delete(np.array(logistic_labels), indexes_to_remove, axis=0)\n",
        "\n",
        "        logistic_regression_results.append(lr.fit(x_data, logistic_labels).coef_)\n",
        "        # print(logistic_regression_results)\n",
        "      except Exception as e:\n",
        "        print(\"Logistic Regression exception occured: \"+str(e))\n",
        "\n",
        "    try:\n",
        "      logistic_regression_matrix = np.vstack((logistic_regression_results[0], logistic_regression_results[1]))\n",
        "      for i in range(logistic_regression_results):\n",
        "        if i > 1 & i + 1 < logistic_regression_results.shape[1]:\n",
        "          logistic_regression_matrix = np.vstack((logistic_regression_matrix, logistic_regression_results[i]))\n",
        "\n",
        "      print(logistic_regression_matrix.shape)\n",
        "    except Exception as e:\n",
        "      print(\"Can not create linear matrix: \"+str(e))\n",
        "# \n",
        "    # if self.i == 0:\n",
        "    #   print(logistic_regression_matrix)\n",
        "    #   self.i += 1\n",
        "\n",
        "  def student_loss_function(self, student_predictions, y):\n",
        "    predicted_class = np.argmax(student_predictions, axis=1)\n",
        "    return -y * np.log(student_predictions)\n",
        "\n",
        "\n",
        "  def fit(self, x_data, y_data, temperature, alpha):\n",
        "\n",
        "    # self.student.fit(x, y, x.shape[1])\n",
        "\n",
        "\n",
        "    self.h1_layer = Dense_Layer(x_data.shape[1], 100)\n",
        "    self.h2_layer = Dense_Layer(100, 50)\n",
        "    self.output_layer = Dense_Layer(50, 3)\n",
        "\n",
        "    step_size = self.eta\n",
        "    reg = 0.02\n",
        "\n",
        "        #Adam params\n",
        "    B1 = 0.9\n",
        "    B2 = 0.999\n",
        "    EPSILON = 1e-10\n",
        "    batch_size = 3#32\n",
        "\n",
        "    self.train_scores = []\n",
        "    self.test_scores = []\n",
        "    i = 0\n",
        "\n",
        "    y_train_one_hot = self.teacher.one_hot(y_data, 3)\n",
        "\n",
        "    for i in range(self.epochs):\n",
        "\n",
        "        indices = np.arange(x_data.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for idx in range(0, indices.shape[0] - batch_size + 1, batch_size):\n",
        "          batch_idx = indices[idx:idx + batch_size]\n",
        "\n",
        "          teacher_predictions = self.teacher.one_hot(self.teacher.predict(x_data[batch_idx]), 3)\n",
        "\n",
        "          z1 = self.h1_layer.forward(x_data[batch_idx])\n",
        "          h1 = self.sigmoid(z1)\n",
        "\n",
        "          z2 = self.h2_layer.forward(h1)\n",
        "          h2 = self.sigmoid(z2)\n",
        "          h2 = self.h2_layer.dropout(h2)\n",
        "\n",
        "          z3 = self.output_layer.output_layer_forward(h2,self.adapter_bias)\n",
        "\n",
        "          dscores = self.softmax(z3, x_data=x_data[batch_idx], y_data=y_data[batch_idx])\n",
        "\n",
        "          student_loss = self.student_loss_function(dscores, y_train_one_hot[batch_idx])\n",
        "\n",
        "          # print('calculate loss')\n",
        "\n",
        "\n",
        "          # distillation_loss = self.softmax(teacher_predictions / temperature) * np.log(self.softmax(teacher_predictions / temperature)\n",
        "          #     /self.softmax(dscores / temperature))\n",
        "          \n",
        "\n",
        "          # print(teacher_predictions.shape)\n",
        "          # print(dscores.shape)\n",
        "          # distillation_loss = (self.softmax(teacher_predictions / temperature) - self.softmax(dscores / temperature)) / temperature\n",
        "          # print('loss calculated')\n",
        "\n",
        "\n",
        "          # loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "          dz_out = dscores - teacher_predictions#dscores - y_train_one_hot[batch_idx]\n",
        "          dw_out = np.dot(h2.T, dz_out)\n",
        "          db_out = np.sum(dscores, axis=0, keepdims=True)\n",
        "\n",
        "          dz2 = np.dot(dz_out, self.output_layer.w.T) * self.derivative_sigmoid(h2)\n",
        "          dz2 = dz2 * self.h2_layer.dropout_mask\n",
        "          dw2 = np.dot(h1.T, dz2)\n",
        "          db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "          dz1 = np.dot(dz2, self.h2_layer.w.T) * self.derivative_sigmoid(h1)\n",
        "          dw1 = np.dot(x_data[batch_idx].T, dz1)\n",
        "          db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "          self.h1_layer.adam_output(B1, B2, EPSILON, dw1, db1, step_size)\n",
        "\n",
        "          self.h2_layer.adam(B1, B2, EPSILON, dw2, db2, step_size)\n",
        "\n",
        "          self.output_layer.adam(B1, B2, EPSILON, dw_out, db_out, step_size)\n",
        "\n",
        "          # distillation_loss = (self.teacher.softmax_activation(teacher_predictions / temperature) - self.student.softmax_activation(student_predictions / temperature)) / temperature\n",
        "          # student_predictions = self.student.predict(x)\n",
        "          # student_loss = self.student_loss_function(student_predictions, y)\n",
        "\n",
        "          # distillation_loss = (self.teacher.softmax_activation(teacher_predictions / temperature) - self.student.softmax_activation(student_predictions / temperature)) / temperature\n",
        "\n",
        "          # loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "  # wracamy do pomysły o podział na trzy osie - this attempt failed!!!\n",
        "  def plot_decision_regions(self, x, y, test_idx=None, resolution=0.1):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    pca = PCA(n_components = 2)\n",
        "    x = pca.fit_transform(x[:, [0, 1]])\n",
        "\n",
        "        # x = x[:, :2]\n",
        "    x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                                np.arange(x2_min, x2_max, resolution))\n",
        "    z = self.predict(np.array([xx1.ravel(), xx2.ravel(), xx3.ravel()]).T)\n",
        "    z = z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "  def plot_compresed_decision_regions(self, x, y, test_idx=None, resolution=0.02):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    \n",
        "\n",
        "    x1_min, x1_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "    x2_min, x2_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                                np.arange(x2_min, x2_max, resolution))\n",
        "    z = self.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    z = z.reshape(xx1.shape)\n",
        "    # xx1 = xx1[:, :, 3]\n",
        "    # xx2 = xx2[:, :, 3]\n",
        "    # z = z[:, :, 3]\n",
        "    # pca = PCA(n_components = 2)\n",
        "    # z = pca.fit_transform(z)\n",
        "\n",
        "    print(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=x[y == cl, 0], y=x[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_4rVjS1jSg5B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 900\n",
        "\n",
        "i=0\n",
        "# while i < 20:\n",
        "  # i+=1\n",
        "destiller = Destiller(atlasNN, eta=0.004, epochs=epochs, adapter_weights=melted_weights, adapter_bias=bias)\n",
        "\n",
        "destiller.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "\n",
        "destiller.plot_compresed_decision_regions(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "O3GceGLlfnjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 900\n",
        "\n",
        "i=0\n",
        "# while i < 20:\n",
        "  # i+=1\n",
        "destiller = Destiller(atlasNN, eta=0.004, epochs=epochs, adapter_weights=melted_weights, adapter_bias=bias)\n",
        "\n",
        "destiller_no = Destiller(atlasNN, eta=0.004, epochs=epochs, adapter_weights=None, adapter_bias=None)\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "destiller.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "destiller.plot_compresed_decision_regions(x_test, y_test)\n",
        "\n",
        "\n",
        "destiller_no.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "print('Train Destiller: ', np.sum(destiller_no.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller: ', np.sum(destiller_no.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "destiller_no.plot_compresed_decision_regions(x_test, y_test)\n"
      ],
      "metadata": {
        "id": "h3Fnxx2-z0Yu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "3118d599-6da9-4037-e906-2222ae8ad2c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-472fea550cf7>:113: RuntimeWarning: invalid value encountered in log\n",
            "  return -y * np.log(student_predictions)\n",
            "<ipython-input-6-1c1901332ddf>:15: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
            "<ipython-input-6-1c1901332ddf>:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Destiller:  0.9444444444444444\n",
            "Test Destiller:  0.9833333333333333\n",
            "(243, 282)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1c1901332ddf>:23: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Train Destiller: ', np.sum(destiller_no.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
            "<ipython-input-6-1c1901332ddf>:24: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  print('Test Destiller: ', np.sum(destiller_no.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Destiller:  0.9333333333333333\n",
            "Test Destiller:  0.9333333333333333\n",
            "(243, 282)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_compressed_decision_regions(model, x, y):\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    pca = PCA(n_components = 1)\n",
        "    #merge 1 and 2\n",
        "\n",
        "    x12 = pca.fit_transform(x[:, [0, 1]])\n",
        "    create_subplot(model, x, y, x12, x[:, 2])\n",
        "    #merge 2 and 3\n",
        "\n",
        "    x23 = pca.fit_transform(x[:, [1, 2]])\n",
        "\n",
        "    #merge 1 and 3\n",
        "\n",
        "    x13 = pca.fit_transform(x[:, [0, 2]])\n",
        "\n",
        "    \n",
        "def create_subplot(model, data, labels, composite_axis, third_axis, resolution=0.2):\n",
        "    x1_min, x1_max = composite_axis[:, 0].min() - 1, composite_axis[:, 0].max() + 1\n",
        "    x2_min, x2_max = third_axis[:].min() - 1, third_axis[:].max() + 1\n",
        "\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    z = model.predict(data)\n",
        "    z = z.reshape(xx1.shape)\n",
        "\n",
        "    # print(xx1.shape)\n",
        "    # print(xx2.shape)\n",
        "    # print(np.array([xx1.ravel(), xx2.ravel(), xx3.ravel()]).T.shape)\n",
        "\n",
        "    plt.contourf(xx1, xx2, z, alpha=0.4, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    a = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=data[y == cl, 0], y=data[y == cl, 1], alpha=0.6, color=cmap(idx), marker=markers[idx], label=a[cl],\n",
        "                        edgecolors='black')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kmraoGo4Z79O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# plot_compressed_decision_regions(destiller, x_test, y_test)\n",
        "\n",
        "print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "\n",
        "print('Train Destiller No Adapter: ', np.sum(destiller_no_adapter.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "print('Test Destiller No Adapter: ', np.sum(destiller_no_adapter.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])"
      ],
      "metadata": {
        "id": "OhIYn-t2ZpBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nowa sekcja"
      ],
      "metadata": {
        "id": "oi-zM0143BCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a)\n",
        "\n",
        "print(np.sum(a, axis = 1, keepdims = True))\n",
        "\n",
        "\n",
        "indexes_to_remove = []\n",
        "logistic_labels = np.array([[1, 0, 0], [1, 1, 1], [0, 0, 1], [1, 1, 1]])\n",
        "for idx, val in enumerate(logistic_labels):\n",
        "  first_label = val[0]\n",
        "  if len(val[val == first_label]) == len(val):\n",
        "    indexes_to_remove.append(idx)\n",
        "\n",
        "\n",
        "# print(np.delete(logistic_labels, indexes_to_remove, axis=0))\n",
        "\n",
        "x_data = [[-0.7795133,   0.08070915,  0.26414192],\n",
        "          [ 2.4920192,   1.50164482,  1.05393502],\n",
        "          [-0.41600969, -1.34022653, -1.3154443 ]]\n",
        "y_data = [1, 2, 0]\n",
        "\n",
        "\n",
        "# epochs = 900\n",
        "\n",
        "# i=0\n",
        "# # while i < 20:\n",
        "#   # i+=1\n",
        "# destiller = Destiller(atlasNN, eta=0.004, epochs=epochs, adapter_weights=melted_weights, adapter_bias=bias)\n",
        "\n",
        "# destiller_no_adapter = Destiller(atlasNN, eta=0.003, epochs=epochs, adapter_weights=None, adapter_bias=bias)\n",
        "\n",
        "# destiller.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "# destiller_no_adapter.fit(x_train, y_train, temperature=3, alpha=0.1)\n",
        "\n",
        "# print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "# print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "\n",
        "# destiller.plot_compresed_decision_regions(x_test, y_test)\n",
        "\n",
        "# # destiller_no_adapter.plot_compresed_decision_regions(x_test, y_test)\n",
        "\n",
        "\n",
        "# # print('Train Destiller: ', np.sum(destiller.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "# # print('Test Destiller: ', np.sum(destiller.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "# print('Train Destiller No Adapter: ', np.sum(destiller_no_adapter.predict(x_train) == y_train).astype(np.float) / x_train.shape[0])\n",
        "# print('Test Destiller No Adapter: ', np.sum(destiller_no_adapter.predict(x_test) == y_test).astype(np.float) / x_test.shape[0])\n",
        "\n",
        "# destiller_no_adapter.plot_compresed_decision_regions(x_test, y_test)\n",
        "\n",
        "\n",
        "logistic_regression_results = []\n",
        "for idx, unique_label in enumerate((np.unique(y_data))):\n",
        "  logistic_labels = []\n",
        "  for i, data_label in enumerate(y_data):\n",
        "    if data_label == unique_label:\n",
        "      logistic_labels.append(1)\n",
        "    else:\n",
        "      logistic_labels.append(0)\n",
        "  try:\n",
        "      print(f'Labels for {unique_label}')\n",
        "      # print(y_data)\n",
        "      print(logistic_labels)\n",
        "      indexes_to_remove = []\n",
        "      print(len(logistic_labels[logistic_labels == 1]))\n",
        "        # if len(val[val == first_label]) == len(val):\n",
        "        #   indexes_to_remove.append(idx)\n",
        "      # filtered_data = np.delete(x_data, indexes_to_remove, axis=0)\n",
        "      # logistic_labels = np.delete(np.array(logistic_labels), indexes_to_remove, axis=0)\n",
        "\n",
        "      # logistic_regression_results.append(lr.fit(filtered_data, logistic_labels).coef_)\n",
        "      # print(logistic_regression_results)\n",
        "  except Exception as e:\n",
        "    print(\"Logistic Regression exception occured: \"+str(e))"
      ],
      "metadata": {
        "id": "6TCS1afS8pgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addition(args):\n",
        "  print(args[1])\n",
        "  return sum(args)\n",
        " \n",
        "print(addition([5, 10, 20, 6]))"
      ],
      "metadata": {
        "id": "oEVJtG75azD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}